# LLM Evaluation Pipeline

This project implements a **lightweight, scalable evaluation pipeline** for analyzing AI-generated responses against provided conversational context.  
It measures **relevance**, **hallucination risk**, **latency**, and **estimated cost** for each evaluated conversation.

---

## ğŸ§  What This Project Does

- Takes **stored chat conversations** (JSON)
- Sends them to an LLM for response generation or evaluation
- Compares the AI response against **retrieved vector context**
- Outputs objective evaluation metrics

This is especially useful for:
- RAG (Retrieval-Augmented Generation) quality checks
- Hallucination monitoring
- AI safety & compliance
- Model regression testing
- Cost & latency tracking at scale
---

````markdown
## âš™ï¸ Local Setup Instructions

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Chrahuldeveloper/llm_evaluator
cd llm_evaluator
````

### 2ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv venv
```

Activate the virtual environment on Windows:

```bash
venv\Scripts\activate
```

Activate the virtual environment on macOS / Linux:

```bash
source venv/bin/activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Create and Configure `.env`

Create a `.env` file in the project root directory:

```env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx
```

### 5ï¸âƒ£ Run the Evaluation Script

```bash
python main.py
```

## ğŸ— Architecture of the Evaluation Pipeline

## Architecture of the Evaluation Pipeline

```text
Chat JSON
   â”‚
   â–¼
Message Builder
   â”‚
   â–¼
LLM Call (OpenAI)
   â”‚
   â”œâ”€â”€ Latency Measurement
   â”œâ”€â”€ Token Usage Extraction
   â”‚
   â–¼
AI Response
   â”‚
   â”œâ”€â”€ Relevance Scoring (Embedding Similarity)
   â”œâ”€â”€ Hallucination Detection (Context Coverage)
   â”‚
   â–¼
Evaluation Metrics Output

```




## ğŸ“ Evaluation Metrics Explained

### 1ï¸âƒ£ Relevance Score
- Uses **SentenceTransformers (MiniLM)**
- Compares AI response embeddings with retrieved context embeddings
- Similarity computed using **cosine similarity**
- Output range:
  - `0.0` â†’ Not relevant
  - `1.0` â†’ Highly relevant

---

### 2ï¸âƒ£ Hallucination Score
- Splits the AI response into individual sentences
- Checks whether each sentence is supported by at least one retrieved context chunk
- Output:
  - `1.0` â†’ No hallucination detected
  - `0.0` â†’ Possible hallucination

---

### 3ï¸âƒ£ Latency
- Measured per LLM API call
- Indicates how long the model took to respond
- Useful for:
  - Performance benchmarking
  - SLA monitoring
  - Real-time system tuning

---

### 4ï¸âƒ£ Cost Estimation
- Based on token usage returned by the OpenAI API
- Uses a configurable **price-per-token**
- Helps:
  - Forecast production costs
  - Monitor budget usage
  - Optimize model selection

---

## ğŸ¤” Why This Design?

### âœ… Why This Approach
- **Decoupled from live chat** â†’ Safe, testable, repeatable
- **Model-agnostic** â†’ Easy to swap LLM providers or models
- **Lightweight embeddings** â†’ Fast inference and low compute cost
- **Explainable metrics** â†’ Simple, interpretable outputs
- **Production-friendly** â†’ Easy to batch, parallelize, and scale

---

### âŒ Why Not Other Approaches

| Approach | Reason Not Used |
|--------|----------------|
| Fine-tuned evaluator model | Expensive to train and maintain |
| Manual human evaluation | Not scalable for large volumes |
| LLM-as-a-judge only | High cost and inconsistent judgments |
| Heavy NLP pipelines | High latency and operational complexity |

---

## ğŸš€ Scaling to Millions of Conversations

### ğŸ”¥ Latency Optimization
- Pre-computed vector embeddings
- Lightweight MiniLM embedding model
- Batch processing of evaluations
- Asynchronous / parallel LLM calls

---

### ğŸ’° Cost Optimization
- Use smaller evaluation models (e.g., `gpt-4o-mini`)
- Cache embeddings and evaluation results
- Threshold-based evaluation (skip high-confidence cases)
- Offline batch evaluation for non-critical paths

---

### ğŸ§  Production Architecture (High Level)
- Message queues (Kafka / SQS)
- Horizontally scaled worker pools
- Result caching (Redis)
- Offline batch jobs for deep audits
- Real-time checks only where required

---

## ğŸ›¡ Security Best Practices
- API keys are stored securely using environment variables via `.env`
- `.env` is excluded from version control using `.gitignore`
- No API keys or secrets are hardcoded in the codebase
- Evaluation pipeline is **read-only** and does not modify user data
- Chat and vector data are processed locally and not persisted beyond evaluation
- only required fields are sent to the LLM

## ğŸ—‚ Project Structure

```text
llm_evaluator/
â”œâ”€â”€ main.py              # Orchestrates evaluation runs
â”œâ”€â”€ evaluator.py         # Core logic: LLM call + scoring
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chat1.json       # Stored chat conversation
â”‚   â”œâ”€â”€ chat2.json
â”‚   â”œâ”€â”€ vector1.json     # Retrieved knowledge chunks
â”‚   â””â”€â”€ vector2.json
â”œâ”€â”€ .env                 # API keys (not committed)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
